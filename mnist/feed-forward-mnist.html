<html>
<div id="text">
</div>
<script src="../js/tf.js"></script>
<script src="../js/jquery-3.3.1.min.js"></script>
<script src="../js/numjs.min.js"></script>
<script src="../js/utils.js"></script>
<script src="../data/mnist.js"> </script>
<script>
$('#text').append("Done load MNIST dataset, total row "+MNIST['data'].length+"<br>");
const dense_layer1 = tf.layers.dense({units: 100, activation: 'relu'});
const dense_layer2 = tf.layers.dense({units: 32, activation: 'relu'});
const dense_layer3 = tf.layers.dense({units: 10, activation: 'linear'});
setTimeout(function(){
  const f = x => dense_layer3.apply(dense_layer2.apply(dense_layer1.apply(x)));
  const cost = (label, pred) => tf.losses.softmaxCrossEntropy(label, pred).mean();
  const accuracy = (label, pred) => tf.cast(tf.equal(label.argMax(1),pred.argMax(1)),'float32').mean();
  const learning_rate = 0.0001;
  const optimizer = tf.train.adam(learning_rate);
  const batch_size = 128;
  const epoch = 1;

  function async_training_loop(callback) {
    (function loop(i) {
      var total_loss = 0, total_acc = 0;
      for(var k = 0; k < Math.floor(MNIST['data'].length/batch_size)*batch_size; k+=batch_size){
        batch_x = tf.tensor(MNIST['data'].slice(k,k+batch_size));
        batch_y = tf.tensor(MNIST['label'].slice(k,k+batch_size));
        total_loss += parseFloat(cost(batch_y, f(batch_x)).toString().slice(7));
        total_acc += parseFloat(accuracy(batch_y, f(batch_x)).toString().slice(7));
        optimizer.minimize(() => cost(batch_y, f(batch_x)));
      }
      total_loss /= Math.floor(MNIST['data'].length/batch_size);
      total_acc /= Math.floor(MNIST['data'].length/batch_size);
      $('#text').append('Epoch: '+(i+1)+', avg loss: '+total_loss+', avg acc: '+total_acc+'<br>');
      if (i < (epoch-1)) {
        setTimeout(function() {loop(++i)}, 1);
      } else {
        callback();
      }
    }(0));
  }
  async_training_loop(function() {
    $('#text').append('Done training!');
  });
}, 5);

</script>
</html>
